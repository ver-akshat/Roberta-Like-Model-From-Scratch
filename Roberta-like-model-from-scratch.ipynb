{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7424587,"sourceType":"datasetVersion","datasetId":4319938}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-19T02:49:58.099491Z","iopub.execute_input":"2024-01-19T02:49:58.099775Z","iopub.status.idle":"2024-01-19T02:49:59.020965Z","shell.execute_reply.started":"2024-01-19T02:49:58.099748Z","shell.execute_reply":"2024-01-19T02:49:59.020067Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/text-data/kant.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Pretraining a RoBERTA like model from scratch:  \n#### Training a tokenizer and pretraining the transformer","metadata":{}},{"cell_type":"code","source":"#imports\nimport accelerate\nfrom accelerate import Accelerator\nfrom tokenizers import ByteLevelBPETokenizer\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:05.559844Z","iopub.execute_input":"2024-01-19T02:50:05.560325Z","iopub.status.idle":"2024-01-19T02:50:10.381015Z","shell.execute_reply.started":"2024-01-19T02:50:05.560294Z","shell.execute_reply":"2024-01-19T02:50:10.380089Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"#### Train the tokenizer using Hugging face BPETokenizer & saving it","metadata":{}},{"cell_type":"code","source":"def train_bpe_tokenizer(train_text, vocab_size=10000):\n    # Initialize the BPE tokenizer\n    tokenizer = ByteLevelBPETokenizer()\n\n    # Train the tokenizer on the provided text\n    tokenizer.train(files=[train_text], vocab_size=vocab_size, min_frequency=2, special_tokens=[\n\"<s>\",\n\"<pad>\",\n\"</s>\",\n\"<unk>\",\n\"<mask>\",\n])\n\n    # directory to save the tokenizer on kaggle output/working folder\n    token_dir = 'RoberTAlikeModel'\n    if not os.path.exists(token_dir):\n        os.makedirs(token_dir)\n    tokenizer.save_model('RoberTAlikeModel')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:13.285522Z","iopub.execute_input":"2024-01-19T02:50:13.286532Z","iopub.status.idle":"2024-01-19T02:50:13.291985Z","shell.execute_reply.started":"2024-01-19T02:50:13.286498Z","shell.execute_reply":"2024-01-19T02:50:13.291117Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Example text for training\ntrain_text = \"/kaggle/input/text-data/kant.txt\"\ntrain_bpe_tokenizer(train_text)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:15.964143Z","iopub.execute_input":"2024-01-19T02:50:15.964624Z","iopub.status.idle":"2024-01-19T02:50:18.309452Z","shell.execute_reply.started":"2024-01-19T02:50:15.964593Z","shell.execute_reply":"2024-01-19T02:50:18.308529Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Loading the tokenizer and then testing on some sample data","metadata":{}},{"cell_type":"code","source":"# Loading the Trained Tokenizer Files\nfrom tokenizers.implementations import ByteLevelBPETokenizer\nfrom tokenizers.processors import BertProcessing\ntokenizer = ByteLevelBPETokenizer(\n\"./RoberTAlikeModel/vocab.json\",\n\"./RoberTAlikeModel/merges.txt\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:18.310993Z","iopub.execute_input":"2024-01-19T02:50:18.311332Z","iopub.status.idle":"2024-01-19T02:50:18.328589Z","shell.execute_reply.started":"2024-01-19T02:50:18.311304Z","shell.execute_reply":"2024-01-19T02:50:18.327631Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"#### Testing the tokenizer on some sample data","metadata":{}},{"cell_type":"code","source":"tokenizer.encode(\"For it is in reality vain to profess in difference in regard to such inquiries, the object of which cannot be indifferent to humanity.\").tokens","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:18.329835Z","iopub.execute_input":"2024-01-19T02:50:18.330186Z","iopub.status.idle":"2024-01-19T02:50:18.342031Z","shell.execute_reply.started":"2024-01-19T02:50:18.330158Z","shell.execute_reply":"2024-01-19T02:50:18.341079Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['For',\n 'Ġit',\n 'Ġis',\n 'Ġin',\n 'Ġreality',\n 'Ġvain',\n 'Ġto',\n 'Ġprofess',\n 'Ġin',\n 'Ġdifference',\n 'Ġin',\n 'Ġregard',\n 'Ġto',\n 'Ġsuch',\n 'Ġinquiries',\n ',',\n 'Ġthe',\n 'Ġobject',\n 'Ġof',\n 'Ġwhich',\n 'Ġcannot',\n 'Ġbe',\n 'Ġindifferent',\n 'Ġto',\n 'Ġhumanity',\n '.']"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.encode(\"For it is in reality vain to profess in difference in regard to such inquiries, the object of which cannot be indifferent to humanity.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:20.028288Z","iopub.execute_input":"2024-01-19T02:50:20.029023Z","iopub.status.idle":"2024-01-19T02:50:20.035291Z","shell.execute_reply.started":"2024-01-19T02:50:20.028986Z","shell.execute_reply":"2024-01-19T02:50:20.034291Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Encoding(num_tokens=26, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"},"metadata":{}}]},{"cell_type":"markdown","source":"#### processing the tokens to fit BERT variant selected, after this tokens will have start and end tokens","metadata":{}},{"cell_type":"code","source":"tokenizer._tokenizer.post_processor = BertProcessing(\n(\"</s>\", tokenizer.token_to_id(\"</s>\")),\n(\"<s>\", tokenizer.token_to_id(\"<s>\")),\n)\ntokenizer.enable_truncation(max_length=512)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:20.991973Z","iopub.execute_input":"2024-01-19T02:50:20.992364Z","iopub.status.idle":"2024-01-19T02:50:20.997934Z","shell.execute_reply.started":"2024-01-19T02:50:20.992333Z","shell.execute_reply":"2024-01-19T02:50:20.996892Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### Example after we process tokenizer","metadata":{}},{"cell_type":"code","source":"#encode a post-processed sequence:\ntokenizer.encode(\"He will achieve his goal.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:24.528003Z","iopub.execute_input":"2024-01-19T02:50:24.528914Z","iopub.status.idle":"2024-01-19T02:50:24.534526Z","shell.execute_reply.started":"2024-01-19T02:50:24.528877Z","shell.execute_reply":"2024-01-19T02:50:24.533515Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.encode(\"He will achieve his goal.\").tokens","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:25.240217Z","iopub.execute_input":"2024-01-19T02:50:25.241117Z","iopub.status.idle":"2024-01-19T02:50:25.247036Z","shell.execute_reply.started":"2024-01-19T02:50:25.241082Z","shell.execute_reply":"2024-01-19T02:50:25.246120Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['<s>', 'He', 'Ġwill', 'Ġachieve', 'Ġhis', 'Ġgoal', '.', '</s>']"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Defining model config: pretraining a RoberTa like transformer using same number of layers and heads as a DistilBert transformer, it has vocab size of 52000,12 attention heads,6 layers","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaConfig\nconfig = RobertaConfig(\n    vocab_size=52_000,\n    max_position_embeddings=514,\n    num_attention_heads=12,\n    num_hidden_layers=6,\n    type_vocab_size=1,\n)\n ","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:27.664000Z","iopub.execute_input":"2024-01-19T02:50:27.664405Z","iopub.status.idle":"2024-01-19T02:50:27.929652Z","shell.execute_reply.started":"2024-01-19T02:50:27.664377Z","shell.execute_reply":"2024-01-19T02:50:27.928844Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(config)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:29.295983Z","iopub.execute_input":"2024-01-19T02:50:29.296426Z","iopub.status.idle":"2024-01-19T02:50:29.302655Z","shell.execute_reply.started":"2024-01-19T02:50:29.296393Z","shell.execute_reply":"2024-01-19T02:50:29.301569Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"RobertaConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.36.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 52000\n}\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Load the trained tokenizer","metadata":{}},{"cell_type":"code","source":"#Re-creating the Tokenizer in Transformers\nfrom transformers import RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"./RoberTAlikeModel\", max_length=512)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:30.583944Z","iopub.execute_input":"2024-01-19T02:50:30.584752Z","iopub.status.idle":"2024-01-19T02:50:30.637028Z","shell.execute_reply.started":"2024-01-19T02:50:30.584718Z","shell.execute_reply":"2024-01-19T02:50:30.636091Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"#### Initialize a model from scratch,examine its size after using the config defined earlier in steps","metadata":{}},{"cell_type":"code","source":"#Initializing a Model From Scratch\nfrom transformers import RobertaForMaskedLM\nmodel = RobertaForMaskedLM(config=config)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:32.956034Z","iopub.execute_input":"2024-01-19T02:50:32.956525Z","iopub.status.idle":"2024-01-19T02:50:34.996429Z","shell.execute_reply.started":"2024-01-19T02:50:32.956488Z","shell.execute_reply":"2024-01-19T02:50:34.995481Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"RobertaForMaskedLM(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-5): 6 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (lm_head): RobertaLMHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (decoder): Linear(in_features=768, out_features=52000, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# just looking at number of parameters in the model\nprint(model.num_parameters())","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:36.664281Z","iopub.execute_input":"2024-01-19T02:50:36.664667Z","iopub.status.idle":"2024-01-19T02:50:36.670548Z","shell.execute_reply.started":"2024-01-19T02:50:36.664638Z","shell.execute_reply":"2024-01-19T02:50:36.669473Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"83504416\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Load dataset line by line to generate samples for batch training","metadata":{}},{"cell_type":"code","source":"#Building the Dataset\nfrom transformers import LineByLineTextDataset\ndataset = LineByLineTextDataset(\ntokenizer=tokenizer,\nfile_path=\"/kaggle/input/text-data/kant.txt\",\nblock_size=128,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:39.443630Z","iopub.execute_input":"2024-01-19T02:50:39.444578Z","iopub.status.idle":"2024-01-19T02:51:19.962155Z","shell.execute_reply.started":"2024-01-19T02:50:39.444528Z","shell.execute_reply":"2024-01-19T02:51:19.961034Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Define data collator: it will take samples from dataset and collate them into batches resulting in dictionary like objects,also preparing a batched sample process for MLM","metadata":{}},{"cell_type":"code","source":"#Defining a data collator\n\nfrom transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)\n  ","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:51:25.947686Z","iopub.execute_input":"2024-01-19T02:51:25.949059Z","iopub.status.idle":"2024-01-19T02:51:25.954490Z","shell.execute_reply.started":"2024-01-19T02:51:25.948994Z","shell.execute_reply":"2024-01-19T02:51:25.953421Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"#### Now the training can be done","metadata":{}},{"cell_type":"code","source":"#Initializing the trainer\n\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./RoberTAlikeModel\",\n    overwrite_output_dir=True,\n    num_train_epochs=2,\n    per_device_train_batch_size=64,\n    save_steps=10_000,\n    save_total_limit=2,\n)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:51:31.015361Z","iopub.execute_input":"2024-01-19T02:51:31.015726Z","iopub.status.idle":"2024-01-19T02:51:32.224269Z","shell.execute_reply.started":"2024-01-19T02:51:31.015696Z","shell.execute_reply":"2024-01-19T02:51:32.223466Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:51:57.568368Z","iopub.execute_input":"2024-01-19T02:51:57.569235Z","iopub.status.idle":"2024-01-19T03:05:13.181150Z","shell.execute_reply.started":"2024-01-19T02:51:57.569201Z","shell.execute_reply":"2024-01-19T03:05:13.180094Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240119_025234-7rldbrnx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/akshat00000verma/huggingface/runs/7rldbrnx' target=\"_blank\">absurd-bird-3</a></strong> to <a href='https://wandb.ai/akshat00000verma/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/akshat00000verma/huggingface' target=\"_blank\">https://wandb.ai/akshat00000verma/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/akshat00000verma/huggingface/runs/7rldbrnx' target=\"_blank\">https://wandb.ai/akshat00000verma/huggingface/runs/7rldbrnx</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5344' max='5344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5344/5344 12:06, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>6.701500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>5.845300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>5.339000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>5.019300</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>4.786800</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>4.593100</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>4.468500</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>4.386600</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>4.297700</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>4.268800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5344, training_loss=4.924521189249918, metrics={'train_runtime': 795.2334, 'train_samples_per_second': 429.972, 'train_steps_per_second': 6.72, 'total_flos': 1896986656521216.0, 'train_loss': 4.924521189249918, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Saving model+tokenizer+config to the disk","metadata":{}},{"cell_type":"code","source":"#Saving the final model (+tokenizer + config) to disk\n\ntrainer.save_model(\"./RoberTAlikeModel\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T03:06:23.124873Z","iopub.execute_input":"2024-01-19T03:06:23.125257Z","iopub.status.idle":"2024-01-19T03:06:23.885232Z","shell.execute_reply.started":"2024-01-19T03:06:23.125225Z","shell.execute_reply":"2024-01-19T03:06:23.884089Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"#### import a language modeling fill-mask task, use the trained model and trained tokenizer to perform MLM","metadata":{}},{"cell_type":"code","source":"#Language modeling with FillMaskPipeline\n\nfrom transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"./RoberTAlikeModel\",\n    tokenizer=\"./RoberTAlikeModel\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T03:06:32.905921Z","iopub.execute_input":"2024-01-19T03:06:32.906809Z","iopub.status.idle":"2024-01-19T03:06:35.255139Z","shell.execute_reply.started":"2024-01-19T03:06:32.906773Z","shell.execute_reply":"2024-01-19T03:06:35.254024Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"fill_mask(\"Human thinking involves human <mask>.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T03:06:38.838211Z","iopub.execute_input":"2024-01-19T03:06:38.838973Z","iopub.status.idle":"2024-01-19T03:06:38.987330Z","shell.execute_reply.started":"2024-01-19T03:06:38.838934Z","shell.execute_reply":"2024-01-19T03:06:38.986284Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"[{'score': 0.24949002265930176,\n  'token': 393,\n  'token_str': ' reason',\n  'sequence': 'Human thinking involves human reason.'},\n {'score': 0.03271394222974777,\n  'token': 611,\n  'token_str': ' cognition',\n  'sequence': 'Human thinking involves human cognition.'},\n {'score': 0.02086200937628746,\n  'token': 605,\n  'token_str': ' conceptions',\n  'sequence': 'Human thinking involves human conceptions.'},\n {'score': 0.019538380205631256,\n  'token': 531,\n  'token_str': ' experience',\n  'sequence': 'Human thinking involves human experience.'},\n {'score': 0.015104752033948898,\n  'token': 722,\n  'token_str': ' laws',\n  'sequence': 'Human thinking involves human laws.'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Goal of this model is to show that we can create datasets to train a transformer for a specific type of complex language modeling task.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}