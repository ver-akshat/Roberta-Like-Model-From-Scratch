{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7424587,"sourceType":"datasetVersion","datasetId":4319938}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-19T02:49:58.099491Z","iopub.execute_input":"2024-01-19T02:49:58.099775Z","iopub.status.idle":"2024-01-19T02:49:59.020965Z","shell.execute_reply.started":"2024-01-19T02:49:58.099748Z","shell.execute_reply":"2024-01-19T02:49:59.020067Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/text-data/kant.txt\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Pretraining a RoBERTA like model from scratch:  \n#### Training a tokenizer and pretraining the transformer","metadata":{}},{"cell_type":"code","source":"#imports\nimport accelerate\nfrom accelerate import Accelerator\nfrom tokenizers import ByteLevelBPETokenizer\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:05.559844Z","iopub.execute_input":"2024-01-19T02:50:05.560325Z","iopub.status.idle":"2024-01-19T02:50:10.381015Z","shell.execute_reply.started":"2024-01-19T02:50:05.560294Z","shell.execute_reply":"2024-01-19T02:50:10.380089Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"#### Train the tokenizer using Hugging face BPETokenizer & saving it","metadata":{}},{"cell_type":"code","source":"def train_bpe_tokenizer(train_text, vocab_size=10000):\n    # Initialize the BPE tokenizer\n    tokenizer = ByteLevelBPETokenizer()\n\n    # Train the tokenizer on the provided text\n    tokenizer.train(files=[train_text], vocab_size=vocab_size, min_frequency=2, special_tokens=[\n\"<s>\",\n\"<pad>\",\n\"</s>\",\n\"<unk>\",\n\"<mask>\",\n])\n\n    # directory to save the tokenizer on kaggle output/working folder\n    token_dir = 'RoberTAlikeModel'\n    if not os.path.exists(token_dir):\n        os.makedirs(token_dir)\n    tokenizer.save_model('RoberTAlikeModel')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:13.285522Z","iopub.execute_input":"2024-01-19T02:50:13.286532Z","iopub.status.idle":"2024-01-19T02:50:13.291985Z","shell.execute_reply.started":"2024-01-19T02:50:13.286498Z","shell.execute_reply":"2024-01-19T02:50:13.291117Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Example text for training\ntrain_text = \"/kaggle/input/text-data/kant.txt\"\ntrain_bpe_tokenizer(train_text)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:15.964143Z","iopub.execute_input":"2024-01-19T02:50:15.964624Z","iopub.status.idle":"2024-01-19T02:50:18.309452Z","shell.execute_reply.started":"2024-01-19T02:50:15.964593Z","shell.execute_reply":"2024-01-19T02:50:18.308529Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Loading the tokenizer and then testing on some sample data","metadata":{}},{"cell_type":"code","source":"# Loading the Trained Tokenizer Files\nfrom tokenizers.implementations import ByteLevelBPETokenizer\nfrom tokenizers.processors import BertProcessing\ntokenizer = ByteLevelBPETokenizer(\n\"./RoberTAlikeModel/vocab.json\",\n\"./RoberTAlikeModel/merges.txt\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:18.310993Z","iopub.execute_input":"2024-01-19T02:50:18.311332Z","iopub.status.idle":"2024-01-19T02:50:18.328589Z","shell.execute_reply.started":"2024-01-19T02:50:18.311304Z","shell.execute_reply":"2024-01-19T02:50:18.327631Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"#### Testing the tokenizer on some sample data","metadata":{}},{"cell_type":"code","source":"tokenizer.encode(\"For it is in reality vain to profess in difference in regard to such inquiries, the object of which cannot be indifferent to humanity.\").tokens","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:18.329835Z","iopub.execute_input":"2024-01-19T02:50:18.330186Z","iopub.status.idle":"2024-01-19T02:50:18.342031Z","shell.execute_reply.started":"2024-01-19T02:50:18.330158Z","shell.execute_reply":"2024-01-19T02:50:18.341079Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['For',\n 'it',\n 'is',\n 'in',\n 'reality',\n 'vain',\n 'to',\n 'profess',\n 'in',\n 'difference',\n 'in',\n 'regard',\n 'to',\n 'such',\n 'inquiries',\n ',',\n 'the',\n 'object',\n 'of',\n 'which',\n 'cannot',\n 'be',\n 'indifferent',\n 'to',\n 'humanity',\n '.']"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.encode(\"For it is in reality vain to profess in difference in regard to such inquiries, the object of which cannot be indifferent to humanity.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:20.028288Z","iopub.execute_input":"2024-01-19T02:50:20.029023Z","iopub.status.idle":"2024-01-19T02:50:20.035291Z","shell.execute_reply.started":"2024-01-19T02:50:20.028986Z","shell.execute_reply":"2024-01-19T02:50:20.034291Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Encoding(num_tokens=26, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"},"metadata":{}}]},{"cell_type":"markdown","source":"#### processing the tokens to fit BERT variant selected, after this tokens will have start and end tokens","metadata":{}},{"cell_type":"code","source":"tokenizer._tokenizer.post_processor = BertProcessing(\n(\"</s>\", tokenizer.token_to_id(\"</s>\")),\n(\"<s>\", tokenizer.token_to_id(\"<s>\")),\n)\ntokenizer.enable_truncation(max_length=512)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:20.991973Z","iopub.execute_input":"2024-01-19T02:50:20.992364Z","iopub.status.idle":"2024-01-19T02:50:20.997934Z","shell.execute_reply.started":"2024-01-19T02:50:20.992333Z","shell.execute_reply":"2024-01-19T02:50:20.996892Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### Example after we process tokenizer","metadata":{}},{"cell_type":"code","source":"#encode a post-processed sequence:\ntokenizer.encode(\"He will achieve his goal.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:24.528003Z","iopub.execute_input":"2024-01-19T02:50:24.528914Z","iopub.status.idle":"2024-01-19T02:50:24.534526Z","shell.execute_reply.started":"2024-01-19T02:50:24.528877Z","shell.execute_reply":"2024-01-19T02:50:24.533515Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.encode(\"He will achieve his goal.\").tokens","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:25.240217Z","iopub.execute_input":"2024-01-19T02:50:25.241117Z","iopub.status.idle":"2024-01-19T02:50:25.247036Z","shell.execute_reply.started":"2024-01-19T02:50:25.241082Z","shell.execute_reply":"2024-01-19T02:50:25.246120Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['<s>', 'He', 'will', 'achieve', 'his', 'goal', '.', '</s>']"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Defining model config: pretraining a RoberTa like transformer using same number of layers and heads as a DistilBert transformer, it has vocab size of 52000,12 attention heads,6 layers","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaConfig\nconfig = RobertaConfig(\n    vocab_size=52_000,\n    max_position_embeddings=514,\n    num_attention_heads=12,\n    num_hidden_layers=6,\n    type_vocab_size=1,\n)\n ","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:27.664000Z","iopub.execute_input":"2024-01-19T02:50:27.664405Z","iopub.status.idle":"2024-01-19T02:50:27.929652Z","shell.execute_reply.started":"2024-01-19T02:50:27.664377Z","shell.execute_reply":"2024-01-19T02:50:27.928844Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"print(config)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:29.295983Z","iopub.execute_input":"2024-01-19T02:50:29.296426Z","iopub.status.idle":"2024-01-19T02:50:29.302655Z","shell.execute_reply.started":"2024-01-19T02:50:29.296393Z","shell.execute_reply":"2024-01-19T02:50:29.301569Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"RobertaConfig {\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"classifier_dropout\": null,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 6,\n  \"pad_token_id\": 1,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.36.2\",\n  \"type_vocab_size\": 1,\n  \"use_cache\": true,\n  \"vocab_size\": 52000\n}\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Load the trained tokenizer","metadata":{}},{"cell_type":"code","source":"#Re-creating the Tokenizer in Transformers\nfrom transformers import RobertaTokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"./RoberTAlikeModel\", max_length=512)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:30.583944Z","iopub.execute_input":"2024-01-19T02:50:30.584752Z","iopub.status.idle":"2024-01-19T02:50:30.637028Z","shell.execute_reply.started":"2024-01-19T02:50:30.584718Z","shell.execute_reply":"2024-01-19T02:50:30.636091Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"#### Initialize a model from scratch,examine its size after using the config defined earlier in steps","metadata":{}},{"cell_type":"code","source":"#Initializing a Model From Scratch\nfrom transformers import RobertaForMaskedLM\nmodel = RobertaForMaskedLM(config=config)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:32.956034Z","iopub.execute_input":"2024-01-19T02:50:32.956525Z","iopub.status.idle":"2024-01-19T02:50:34.996429Z","shell.execute_reply.started":"2024-01-19T02:50:32.956488Z","shell.execute_reply":"2024-01-19T02:50:34.995481Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"RobertaForMaskedLM(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-5): 6 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (lm_head): RobertaLMHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (decoder): Linear(in_features=768, out_features=52000, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# just looking at number of parameters in the model\nprint(model.num_parameters())","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:36.664281Z","iopub.execute_input":"2024-01-19T02:50:36.664667Z","iopub.status.idle":"2024-01-19T02:50:36.670548Z","shell.execute_reply.started":"2024-01-19T02:50:36.664638Z","shell.execute_reply":"2024-01-19T02:50:36.669473Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"83504416\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Load dataset line by line to generate samples for batch training","metadata":{}},{"cell_type":"code","source":"#Building the Dataset\nfrom transformers import LineByLineTextDataset\ndataset = LineByLineTextDataset(\ntokenizer=tokenizer,\nfile_path=\"/kaggle/input/text-data/kant.txt\",\nblock_size=128,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:50:39.443630Z","iopub.execute_input":"2024-01-19T02:50:39.444578Z","iopub.status.idle":"2024-01-19T02:51:19.962155Z","shell.execute_reply.started":"2024-01-19T02:50:39.444528Z","shell.execute_reply":"2024-01-19T02:51:19.961034Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the  Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Define data collator: it will take samples from dataset and collate them into batches resulting in dictionary like objects,also preparing a batched sample process for MLM","metadata":{}},{"cell_type":"code","source":"#Defining a data collator\n\nfrom transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)\n  ","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:51:25.947686Z","iopub.execute_input":"2024-01-19T02:51:25.949059Z","iopub.status.idle":"2024-01-19T02:51:25.954490Z","shell.execute_reply.started":"2024-01-19T02:51:25.948994Z","shell.execute_reply":"2024-01-19T02:51:25.953421Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"#### Now the training can be done","metadata":{}},{"cell_type":"code","source":"#Initializing the trainer\n\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./RoberTAlikeModel\",\n    overwrite_output_dir=True,\n    num_train_epochs=2,\n    per_device_train_batch_size=64,\n    save_steps=10_000,\n    save_total_limit=2,\n)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:51:31.015361Z","iopub.execute_input":"2024-01-19T02:51:31.015726Z","iopub.status.idle":"2024-01-19T02:51:32.224269Z","shell.execute_reply.started":"2024-01-19T02:51:31.015696Z","shell.execute_reply":"2024-01-19T02:51:32.223466Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-19T02:51:57.568368Z","iopub.execute_input":"2024-01-19T02:51:57.569235Z","iopub.status.idle":"2024-01-19T03:05:13.181150Z","shell.execute_reply.started":"2024-01-19T02:51:57.569201Z","shell.execute_reply":"2024-01-19T03:05:13.180094Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240119_025234-7rldbrnx</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/akshat00000verma/huggingface/runs/7rldbrnx' target=\"_blank\">absurd-bird-3</a></strong> to <a href='https://wandb.ai/akshat00000verma/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/akshat00000verma/huggingface' target=\"_blank\">https://wandb.ai/akshat00000verma/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/akshat00000verma/huggingface/runs/7rldbrnx' target=\"_blank\">https://wandb.ai/akshat00000verma/huggingface/runs/7rldbrnx</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5344' max='5344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5344/5344 12:06, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>6.701500</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>5.845300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>5.339000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>5.019300</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>4.786800</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>4.593100</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>4.468500</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>4.386600</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>4.297700</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>4.268800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=5344, training_loss=4.924521189249918, metrics={'train_runtime': 795.2334, 'train_samples_per_second': 429.972, 'train_steps_per_second': 6.72, 'total_flos': 1896986656521216.0, 'train_loss': 4.924521189249918, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Saving model+tokenizer+config to the disk","metadata":{}},{"cell_type":"code","source":"#Saving the final model (+tokenizer + config) to disk\n\ntrainer.save_model(\"./RoberTAlikeModel\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T03:06:23.124873Z","iopub.execute_input":"2024-01-19T03:06:23.125257Z","iopub.status.idle":"2024-01-19T03:06:23.885232Z","shell.execute_reply.started":"2024-01-19T03:06:23.125225Z","shell.execute_reply":"2024-01-19T03:06:23.884089Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"#### import a language modeling fill-mask task, use the trained model and trained tokenizer to perform MLM","metadata":{}},{"cell_type":"code","source":"#Language modeling with FillMaskPipeline\n\nfrom transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"./RoberTAlikeModel\",\n    tokenizer=\"./RoberTAlikeModel\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T03:06:32.905921Z","iopub.execute_input":"2024-01-19T03:06:32.906809Z","iopub.status.idle":"2024-01-19T03:06:35.255139Z","shell.execute_reply.started":"2024-01-19T03:06:32.906773Z","shell.execute_reply":"2024-01-19T03:06:35.254024Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"fill_mask(\"Human thinking involves human <mask>.\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T03:06:38.838211Z","iopub.execute_input":"2024-01-19T03:06:38.838973Z","iopub.status.idle":"2024-01-19T03:06:38.987330Z","shell.execute_reply.started":"2024-01-19T03:06:38.838934Z","shell.execute_reply":"2024-01-19T03:06:38.986284Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"[{'score': 0.24949002265930176,\n  'token': 393,\n  'token_str': ' reason',\n  'sequence': 'Human thinking involves human reason.'},\n {'score': 0.03271394222974777,\n  'token': 611,\n  'token_str': ' cognition',\n  'sequence': 'Human thinking involves human cognition.'},\n {'score': 0.02086200937628746,\n  'token': 605,\n  'token_str': ' conceptions',\n  'sequence': 'Human thinking involves human conceptions.'},\n {'score': 0.019538380205631256,\n  'token': 531,\n  'token_str': ' experience',\n  'sequence': 'Human thinking involves human experience.'},\n {'score': 0.015104752033948898,\n  'token': 722,\n  'token_str': ' laws',\n  'sequence': 'Human thinking involves human laws.'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Goal of this model is to show that we can create datasets to train a transformer for a specific type of complex language modeling task.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}